---
title: "K-moyennes"
author: "Guyliann Engels, Raphael Conotte & Philippe Grosjean"
description: "**SDD II Module 6** Regroupement par les K-moyennes"
tutorial:
  id: "B06La_kmeans"
  version: 2.1.0/4
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
BioDataScience2::learnr_setup()
# k_means for SciViews, version 1.0.0
# Copyright (c) 2021, Philippe Grosjean (phgrosjean@sciviews.org)

SciViews::R()
library(broom)

# scale() is a generic function, but it does not provide a method for data
# frames. As such, data frames and tibbles are converted into matrices by the
# default method, which is not what we want
scale.data.frame <- function(x, center = TRUE, scale = TRUE)
  as.data.frame(scale(as.matrix(x)))
# This is for tibbles
scale.tbl_df <- function(x, center = TRUE, scale = TRUE)
  tibble::as_tibble(scale(as.matrix(x)))
# This is for data.tables
scale.data.table <- function(x, center = TRUE, scale = TRUE)
  data.table::as.data.table(scale(as.matrix(x)))

# This is a reworked version of factoextra::fviz_nbclust() to help chosing the
# number of clusters for kmeans()
profile_k <- function(x, FUNcluster = kmeans, method = "wss", k.max = NULL, ...) {
  if (NROW(x) < 2)
    stop("You must provide an data frame or matrix with at least two rows")
  if (is.null(k.max))
    k.max <- min(nrow(x) - 1, 10) # Avoid error with very small datasets in fviz_nbclust()
  factoextra::fviz_nbclust(x, FUNcluster = FUNcluster, method = method,
    k.max = k.max, ...)
}

# Traditional kmeans does not store the data... and this is a problem for plot
# later on. So, we define k_means() which store the original data by default
k_means <- function(x, k, centers = k, iter.max = 10L, nstart = 1L,
algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"), trace = FALSE,
keep.data = TRUE) {
  # k and centers are synonyms
  res <- kmeans(x, centers = centers, iter.max = iter.max, nstart = nstart,
    algorithm = algorithm, trace = trace)
  if (isTRUE(keep.data))
    res$data <- as.data.frame(x)
  class(res) <- unique(c("k_means", class(res)))
  res
}

# broom::augment.kmeans() seems buggy when data is called 'x'
augment.kmeans <- function(object, data) {
  res <- broom::fix_data_frame(data, newcol = ".rownames")
  res$.cluster <- factor(object$cluster)
  res
}

# No predict() method for kmeans, but we add one for k_means
predict.k_means <- function(object, ...)
  factor(object$cluster)

# There is no plot, autoplot and chart methods for kmeans objects => make them
# for k_means objects, because we have both the k-means results and the data
# Since data is no contained in the kmeans object, one has to provide it also
plot.k_means <- function(x, y, data = x$data, choices = 1L:2L,
  col = NULL, c.shape = 8, c.size = 3, ...) {
  nclust <- nrow(x$centers)
  if (is.null(col))
    col <- 1:nclust
  plot(as.data.frame(data)[, choices], col = col[x$cluster], ...)
  points(as.data.frame(x$centers)[, choices], col = col[1:nclust],
    pch = c.shape, cex = c.size)
}

autoplot.k_means <- function(object, data = object$data, choices = 1L:2L,
alpha = 1, c.shape = 8, c.size = 3, theme = NULL, use.chart = FALSE, ...) {
  data <- as.data.frame(data)
  vars <- choices
  if (is.numeric(choices))
    vars <- colnames(data)[choices]
  var_x <- as.name(vars[1])
  var_y <- as.name(vars[2])
  centers <- broom::tidy(object, col.names = colnames(data))
  cluster <- factor(object$cluster)
  if (isTRUE(use.chart)) {
    fun <- chart::chart
  } else {
    fun <- ggplot2::ggplot
  }
  res <- fun(data = data, mapping = aes(x = {{var_x}}, y = {{var_y}},
    col  = cluster)) +
    geom_point(alpha = alpha) +
    geom_point(data = centers, size = c.size, shape = c.shape)
  if (!is.null(theme))
    res <- res + theme
  res
}

chart.k_means <- function(data, ..., type = NULL, env = parent.frame())
  autoplot(data, type = type, theme = theme_sciviews(), use.chart = TRUE, ...)


# Preparation dataset
data("doubs", package = "ade4")
envir <- as_tibble(doubs$env)
```

```{r, echo=FALSE}
BioDataScience2::learnr_banner()
```

```{r, context="server"}
BioDataScience2::learnr_server(input, output, session)
```

----

## Objectifs

La méthode des k-moyennes ("k-means" en anglais) permet de réaliser des regroupements d'individus en partant d'un tableau multivarié. Elle a donc le même objectif que la classification hiérarchique ascendante (CAH) que vous avez découverte dans le module précédent. La méthode des k-moyennes a l'avantage, par rapport à la classification ascendante hiérarchique, d'être moins gourmande en temps de calcul et en mémoire vive. Elle sera donc à privilégier lorsque vous vous retrouvez confronté à un grand jeu de données. Par contre, elle est souvent moins efficace que la CAH.

Dans ce tutoriel, vous allez pouvoir auto-évaluer votre capacité à\ :

- comprendre les différentes étapes de la classification par les k-moyennes\ : choix du nombre de groupes, classification par les k-moyennes, récupération des coordonnées des centres et représentation graphique.
- analyser et interpréter de manière autonome un jeu de données multivariées à l'aide des k-moyennes

N'entamez ce tutoriel qu'après avoir compris le principe des k-moyennes proposé dans le [module 6](https://wp.sciviews.org/sdd-umons2/?iframe=wp.sciviews.org/sdd-umons2-2020/k-moyenne-mds-som.html) du cours et en particulier la [section 6.1](https://wp.sciviews.org/sdd-umons2/?iframe=wp.sciviews.org/sdd-umons2-2020/k-moyennes.html). Assurez-vous d'avoir réalisé les exercices H5P qui s'y trouvent avant de vous lancer dans ce tutoriel-ci.

## Données environnementales du Doubs

Nous utilisons ici le même jeu de données concernant les variables environnementales mesurées à 30 stations le long du Doubs, une rivière qui serpente entre la France et la Suisse. Il ne s'agit pas d'un gros jeu de données, et vous avez pu constater que la CAH s'y applique facilement. Néanmoins, nous le conservons à des fins de comparaison entre les deux techniques k-moyennes et CAH.

![Carte du Doubs, d'après OpenStreetMap.](images/doubs.jpg){width='60%'} 

Pour rappel, le tableau suivant reprend les onze variables mesurées. Rappelez-vous que les auteurs de l'étude d'où sont issues ces données ont décidé d'appliquer des coefficients multiplicateurs pour homogénéiser les données\ : 

| label   | description                      | unités * coef |
|:-------:|:---------------------------------|:--------------|
| **dfs** | distance depuis la source        | km * 10       |
| **alt** | altitude                         | m             |
| **slo** | pente des berges en `log(x + 1)` | ‰ * 100       |
| **flo** | flux moyen minimum               | m^3^/s * 100  |
| **pH**  | pH de l'eau                      | - * 10        |
| **har** | dureté totale de l'eau           | mg Ca^++^/L   | 
| **pho** | phosphates                       | mg/L * 100    |
| **nit** | nitrates                         | mg/L * 100    |
| **amm** | azote ammoniacal                 | mg/L * 100    |
| **oxy** | oxygène dissout                  | mg/L * 10     |
| **bdo** | demande biologique en oxygène    | mg/L * 10     |

Voici les premières lignes du tableau `envir` qui contient ces données.

```{r}
head(envir)
```

## Choix du nombre de clusters

Avec la méthode des k-moyennes, vous devez spécifier le nombre de groupes que vous souhaitez réaliser à l'avance. Souvent, à ce stade de l'analyse, il est inconnu. Vous pouvez utiliser `profile_k()` pour vous aider à choisir la valeur de *k* qui diminuera le plus la somme des carrés des distances intra-groupes, comme indicateur de la qualité de votre regroupement.

Sur base du jeu de données `envir`, réalisez un graphique permettant d'estimer le nombre de groupes à employer dans la méthode des k-moyennes. Les variables mesurées ayant des unités différentes, n'oubliez pas de **standardiser** vos données en utilisant la fonction `scale()` et assignez les ensuite à `envir_scale`. Ce n'est pas le cas pour ce jeu de données, mais pensez aussi à éliminer les colonnes non numériques à l’aide de `select()` si vous en avez dans votre tableau de départ.

```{r nbclust_h2, exercise=TRUE}
envir_scale <- ___(___)
___(___)
```

```{r nbclust_h2-hint-1}
envir_scale <- ___(envir)
___(envir_scale)

#### ATTENTION: Hint suivant = solution !####
```

```{r nbclust_h2-solution}
envir_scale <- scale(envir)
profile_k(envir_scale)
```

```{r nbclust_h2-check}
grade_code("Avec de ce graphique, vous avez un outil qui peut vous aider à déterminer le nombre k de centres à utiliser. La valeur obtenue pour *k* = 1 nous indique de combien les données sont dispersées. En augmentant la valeur de *k*, l'objectif est de faire des regroupements pour diminuer cette variance intra-groupe, mais pas au delà d'une valeur qui ne diminuerait plus de manière importante.")
```

```{r qu_nbclust}
question("Sur base du graphique que vous avez réalisé, quelle valeur de k utiliseriez-vous ?",
  answer("1"),
  answer("2"),
  answer("3"),
  answer("4", correct = TRUE),
  answer("5"),
  answer("Plus de 5"), 
  allow_retry = TRUE,
  correct = "Bravo, vous avez trouvé la bonne réponse. Sur le graphique, on choisira une valeur de *k* à la base du coude, lorsque l'ajout d'un *k* supplémentaire ne permet plus de faire baisser la valeur de manière importante.",
  incorrect = "Retentez votre chance. Nous recherchons des sauts importants dans la décroissance de la somme des carrés (*total within sum of square*). L'objectif est de choisir la valeur de *k* à la base du coude, lorsque l'ajout d'un *k* supplémentaire ne permet plus de faire baisser la somme des carrés de manière importante.")
```

## Calcul des k-moyennes

Maintenant que vous avez défini la valeur de *k* que vous allez utiliser, vous pouvez procéder au calcul des k-moyennes en utilisant la fonction `k_means()`. Vous lui fournirez le tableau de départ contenant uniquement des valeurs numériques éventuellement *standardisées* et spécifierez le nombre `k =` de groupes souhaités.

La position initiale des k centres est déterminée aléatoirement. Le résultat final peut donc varier et ne pas être optimal. Pour éviter cela, nous pouvons utiliser l’argument `nstart =` qui testera différentes situations de départ pour conserver le meilleur résultat. Par défaut, une seule situation aléatoire de départ `nstart = 1` est considérée. Avec une valeur plus importante de `nstart =`, votre analyse sera plus robuste… mais cela augmentera le temps de calcul.

En utilisant le tableau standardisé mis à votre disposition et enregistré sous `envir_scale`, réalisez un regroupement avec la fonction `k_means()`. Utilisez pour cela une valeur *k* de 4 et 25 positions de départ différentes. 

```{r kmeans_prep}
envir_scale <- as_tibble(scale(envir))
```

```{r kmeans_h2, exercise=TRUE, exercise.setup="kmeans_prep"}
set.seed(210219) # Point de départ du générateur pseudo-aléatoire
(envir_kmn <- ___(___, ___ = ___, nstart = ___))
```

```{r kmeans_h2-hint-1}
set.seed(210219) # Point de départ du générateur pseudo-aléatoire
(envir_kmn <- ___(envir_scale, ___ = 4, nstart = 25))

#### ATTENTION: Hint suivant = solution !####
```

```{r kmeans_h2-solution}
set.seed(210219) # Point de départ du générateur pseudo-aléatoire
(envir_kmn <- k_means(envir_scale, k = 4, nstart = 25))
```

```{r kmeans_h2-check}
grade_code("Vous venez de réaliser votre première classification par les k moyennes ! La sortie que vous obtenez est l'impression du contenu de l'objet k_means qui vous donne beaucoup d'information. Vous avez le nombre de stations présentes dans chaque groupe (2, 12, 6 et 10). Ensuite, la section \"Cluster means\" reprend les positions des centres pour les 4 groupes que vous avez réalisé. Une mesure de la qualité de regroupement de vos données est présentée dans \"Within cluster sum of squares\", 69.3% ici. Plus cette valeur sera proche de 100% mieux ce sera !")
```

### Graphique du regroupement

Réaliser un graphique des nitrates `nit` en fonction de l'oxygène dissout `oxy`. Regrouper les stations par de la couleur en utilisant les groupes que vous avez calculé et représentez les centres sur votre graphique.

```{r graphe_prep}
envir_scale <- as_tibble(scale(envir))
set.seed(210219) 
envir_kmn <- k_means(envir_scale, k = 4, nstart = 25)
```

```{r graphe_h3, exercise=TRUE, exercise.setup="graphe_prep"}
___(___, ___ = ___)
```

```{r graphe_h3-hint-1}
chart(___, ___ = c(___, ___))
```

```{r graphe_h3-hint-2}
chart(___, choices = c("___", "___"))

#### ATTENTION: Hint suivant = solution !####
```

```{r graphe_h3-solution}
chart(envir_kmn, choices = c("oxy", "nit"))
```

```{r graphe_h3-check}
grade_code("Vous voyez comme c'est facile ! Les fonctions SciViews vous facilitent la tâche. Naturellement, vous pouvez aussi choisir d'autres paires de variables pour visualiser votre regroupement.")
```

## Conclusion

Vous venez de terminer votre auto-évaluation relative à la classification par les k-moyenne. Vous êtes maintenant prêts pour appliquer cette technique sur d'autres données par vous-même dans une assignation GitHub.

```{r comm_noscore, echo=FALSE}
question_text(
  "Laissez-nous vos impressions sur cet outil pédagogique",
  answer("", TRUE, message = "Pas de commentaires... C'est bien aussi."),
  incorrect = "Vos commentaires sont enregistrés.",
  placeholder = "Entrez vos commentaires ici...",
  allow_retry = TRUE
)
```
